"""
íš¨ìœ¨ì ì¸ ëª¨ë¸ ê´€ë¦¬ì - ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ì¬ì‚¬ìš©
"""

import torch
import os
from typing import Optional, Dict, Any
import threading
import time

try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
except ImportError:
    from transformers import AutoModelForCausalLM as Qwen2VLForConditionalGeneration, AutoProcessor

class ModelManager:
    """
    ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    í•œ ë²ˆ ë¡œë“œëœ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ê³  ì¬ì‚¬ìš©
    """
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.models = {}  # {model_id: {'model': model, 'processor': processor, 'device': device, 'last_used': time}}
            self.current_model_id = None
            self.max_models = 2  # ìµœëŒ€ 2ê°œ ëª¨ë¸ê¹Œì§€ ë©”ëª¨ë¦¬ì— ìœ ì§€
            self.initialized = True
            print("ğŸ”§ ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_device(self, device="auto"):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                    test_tensor = torch.tensor([1.0]).cuda()
                    del test_tensor
                    torch.cuda.empty_cache()
                    return "cuda"
                except Exception:
                    return "cpu"
            else:
                return "cpu"
        return device
    
    def get_model(self, model_id: str, device: str = "auto", force_reload: bool = False):
        """
        ëª¨ë¸ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ë¡œë“œí•¨
        
        Args:
            model_id: ëª¨ë¸ ì‹ë³„ì (ì˜ˆ: "Qwen/Qwen2.5-VL-3B-Instruct")
            device: ë””ë°”ì´ìŠ¤ ("auto", "cuda", "cpu")
            force_reload: ê°•ì œë¡œ ë‹¤ì‹œ ë¡œë“œí• ì§€ ì—¬ë¶€
        
        Returns:
            (model, processor, actual_device) íŠœí”Œ
        """
        actual_device = self._get_device(device)
        cache_key = f"{model_id}_{actual_device}"
        
        # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ ìˆê³  ê°•ì œ ë¦¬ë¡œë“œê°€ ì•„ë‹ˆë©´ ì¬ì‚¬ìš©
        if cache_key in self.models and not force_reload:
            model_info = self.models[cache_key]
            model_info['last_used'] = time.time()
            self.current_model_id = cache_key
            
            print(f"â™»ï¸  ê¸°ì¡´ ëª¨ë¸ ì¬ì‚¬ìš©: {model_id}")
            print(f"ğŸ“ ë””ë°”ì´ìŠ¤: {actual_device}")
            
            return model_info['model'], model_info['processor'], actual_device
        
        # ìƒˆë¡œ ë¡œë“œí•´ì•¼ í•˜ëŠ” ê²½ìš°
        print(f"ğŸ”„ ìƒˆ ëª¨ë¸ ë¡œë”©: {model_id}")
        print(f"ğŸ“ ë””ë°”ì´ìŠ¤: {actual_device}")
        
        if actual_device == "cpu":
            print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("â³ ëª¨ë¸ ë¡œë”©ì— ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”ì‹œ)
            self._cleanup_old_models()
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            print("ğŸ“¦ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
            processor = AutoProcessor.from_pretrained(
                model_id,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘...")
            if actual_device == "cpu":
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_id,
                    torch_dtype=torch.float32,
                    device_map=None,
                    trust_remote_code=True
                )
                model = model.to("cpu")
            else:
                model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            # ìºì‹œì— ì €ì¥
            self.models[cache_key] = {
                'model': model,
                'processor': processor,
                'device': actual_device,
                'last_used': time.time(),
                'model_id': model_id
            }
            
            self.current_model_id = cache_key
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return model, processor, actual_device
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # GPU ì‹¤íŒ¨ ì‹œ CPUë¡œ ì¬ì‹œë„
            if actual_device == "cuda":
                print("â™¾ï¸ CPU ëª¨ë“œë¡œ ì¬ì‹œë„...")
                return self.get_model(model_id, "cpu", force_reload)
            
            raise e
    
    def _cleanup_old_models(self):
        """ì˜¤ë˜ëœ ëª¨ë¸ë“¤ì„ ë©”ëª¨ë¦¬ì—ì„œ ì •ë¦¬"""
        if len(self.models) >= self.max_models:
            # ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ ëª¨ë¸ ì°¾ê¸°
            oldest_key = min(self.models.keys(), 
                           key=lambda k: self.models[k]['last_used'])
            
            print(f"ğŸ§¹ ì˜¤ë˜ëœ ëª¨ë¸ ì •ë¦¬: {self.models[oldest_key]['model_id']}")
            
            # ëª¨ë¸ ì •ë¦¬
            model_info = self.models[oldest_key]
            del model_info['model']
            del model_info['processor']
            del self.models[oldest_key]
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def list_loaded_models(self):
        """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ë“¤ì˜ ì •ë³´ ë°˜í™˜"""
        result = []
        for cache_key, model_info in self.models.items():
            result.append({
                'cache_key': cache_key,
                'model_id': model_info['model_id'],
                'device': model_info['device'],
                'last_used': model_info['last_used'],
                'is_current': cache_key == self.current_model_id
            })
        return result
    
    def clear_all_models(self):
        """ëª¨ë“  ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ ì •ë¦¬"""
        print("ğŸ§¹ ëª¨ë“  ëª¨ë¸ ì •ë¦¬ ì¤‘...")
        for model_info in self.models.values():
            del model_info['model']
            del model_info['processor']
        
        self.models.clear()
        self.current_model_id = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ… ëª¨ë“  ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ")
    
    def get_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
        info = {
            'loaded_models': len(self.models),
            'max_models': self.max_models,
            'current_model': self.current_model_id
        }
        
        if torch.cuda.is_available():
            info['gpu_memory_allocated'] = torch.cuda.memory_allocated() / 1024**3  # GB
            info['gpu_memory_reserved'] = torch.cuda.memory_reserved() / 1024**3    # GB
        
        return info
    
    def change_max_models(self, max_models: int):
        """ìµœëŒ€ ëª¨ë¸ ìˆ˜ ë³€ê²½"""
        old_max = self.max_models
        self.max_models = max_models
        
        # í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ì´ ìƒˆë¡œìš´ ìµœëŒ€ê°’ë³´ë‹¤ ë§ìœ¼ë©´ ì •ë¦¬
        while len(self.models) > self.max_models:
            self._cleanup_old_models()
        
        print(f"ğŸ“Š ìµœëŒ€ ëª¨ë¸ ìˆ˜ ë³€ê²½: {old_max} â†’ {max_models}")


# ì „ì—­ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
model_manager = ModelManager()


def get_model_manager():
    """ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return model_manager


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ëª¨ë¸ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    
    manager = get_model_manager()
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:", manager.get_memory_usage())
    
    # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
    print("ë¡œë“œëœ ëª¨ë¸ë“¤:", manager.list_loaded_models())
