"""
Qwen Cloud APIë¥¼ ì‚¬ìš©í•œ OCR ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
"""

import dashscope
import base64
import os
from PIL import Image
from tqdm import tqdm
import time
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from utils import create_output_directory, draw_text_on_image, save_text_result, measure_time
from network_utils import configure_ssl, create_robust_session
from network_advanced import create_permissive_session, configure_advanced_ssl
from endpoint_config import configure_international_endpoint
from response_utils import extract_text_from_response, debug_response_structure

class CloudOCRProcessor:
    def __init__(self, api_key, model_name="qwen-vl-plus"):
        self.api_key = api_key
        self.model_name = model_name
        
        # êµ­ì œ ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • ë¨¼ì €
        configure_international_endpoint()
        
        # dashscope API í‚¤ ì„¤ì •
        dashscope.api_key = api_key
        
        # SSL ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì • (ê´€ëŒ€í•œ ëª¨ë“œ)
        configure_advanced_ssl()
        self.session = create_permissive_session()
        
        # OCR ëª¨ë“œ ì„¤ì • (shape_detection, general, hybrid)
        self.ocr_mode = "shape_detection"
    
    def set_ocr_mode(self, mode):
        """ì¶”ì¶œ ëª¨ë“œ ì„¤ì •"""
        if mode in ["shape_detection", "general", "hybrid"]:
            self.ocr_mode = mode
            print(f"ğŸ”„ OCR ëª¨ë“œ ë³€ê²½: {mode}")
        else:
            print(f"âš ï¸  ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë“œ: {mode}. shape_detection, general, hybridë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        
    def _encode_image(self, image_path):
        """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
            return None
    
    def _get_prompt_for_mode(self, mode="shape_detection"):
        """OCR ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ - ìˆ˜ê¸° ë„í˜• í…ìŠ¤íŠ¸ ìµœì í™”"""
        if mode == "shape_detection":
            return """ì´ ìŠ¤ìº” ë¬¸ì„œì—ì„œ ìˆ˜ê¸°ë¡œ ê·¸ì–´ì§„ ë„í˜•(ì›, íƒ€ì›) ì•ˆì— ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”.

ğŸ¯ í•µì‹¬ ì‘ì—…:
1. ì†ìœ¼ë¡œ ê·¸ë¦° ë„í˜•ë“¤ì„ ì‹ë³„í•˜ì„¸ìš” (ì™„ì „í•˜ì§€ ì•Šê±°ë‚˜ ë¶ˆê·œì¹™í•œ ëª¨ì–‘ í¬í•¨)
2. ê° ë„í˜• ë‚´ë¶€ì— ìˆëŠ” ì†ê¸€ì”¨ë‚˜ ì¸ì‡„ëœ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”
3. ë„í˜•ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ë¹ ì§ì—†ì´ ì¶”ì¶œí•´ì£¼ì„¸ìš”

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ì†ê·¸ë¦¼ ë„í˜•ì€ ë¶ˆì™„ì „í•˜ê³  ì°Œê·¸ëŸ¬ì ¸ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì›í˜•, íƒ€ì›í˜• í˜•íƒœê°€ ìˆìŠµë‹ˆë‹¤
- ë„í˜•ì´ ê²¹ì¹˜ê±°ë‚˜ ë¶™ì–´ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë„í˜• í¬ê¸°ëŠ” ë§¤ìš° ë‹¤ì–‘í•©ë‹ˆë‹¤ (ì‘ì€ ê²ƒë¶€í„° í° ê²ƒê¹Œì§€)
- í”„ë¦°íŠ¸ëœ í…ìŠ¤íŠ¸ì™€ ì†ê¸€ì”¨ê°€ í˜¼ì¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë„í˜• ê²½ê³„ì„ ê³¼ ë‚´ë¶€ í…ìŠ¤íŠ¸ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
- íë¦¿í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ê¸€ìë„ ìµœëŒ€í•œ ì¶”ì¸¡í•´ì„œ ì½ì–´ì£¼ì„¸ìš”

ğŸ“ ì¶œë ¥ í˜•ì‹:
ê° ë„í˜•ì—ì„œ ë°œê²¬ëœ í…ìŠ¤íŠ¸ë¥¼ í•œ ì¤„ì”© ì¶œë ¥í•´ì£¼ì„¸ìš”.
ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì—†ì´ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”."""
        elif mode == "hybrid":
            return """ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”. (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ - ë„í˜• ì˜ì—­ ì„¸ë¶„í™” ì²˜ë¦¬)

ğŸ¯ ì§€ì¹¨:
- ì´ ì´ë¯¸ì§€ëŠ” íŠ¹ì • ë„í˜• ì˜ì—­ì—ì„œ ì˜ë¼ë‚¸ ë¶€ë¶„ì…ë‹ˆë‹¤
- ëª¨ë“  ê¸€ìì™€ ìˆ«ìë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”
- ì†ê¸€ì”¨ì™€ ì¸ì‡„ í…ìŠ¤íŠ¸ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤
- íë¦¿í•˜ê±°ë‚˜ ë¶€ë¶„ì ìœ¼ë¡œ ì˜ë¦° ê¸€ìë„ ìµœëŒ€í•œ ì¶”ì¸¡í•´ì„œ ì½ì–´ì£¼ì„¸ìš”
- ì¢Œí‘œë‚˜ ìœ„ì¹˜ ì •ë³´ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”

ğŸ“ ì¶œë ¥ í˜•ì‹:
- ì„¤ëª…ì´ë‚˜ ì£¼ì„ ì—†ì´ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”
- í•œ ì¤„ì— í•˜ë‚˜ì˜ ë‹¨ì–´ë‚˜ êµ¬ë¬¸ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”

Read all visible text in this cropped region. Focus on handwritten and printed text. Return only the actual text content."""
        else:  # general mode
            return """ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì„œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì§€ì¹¨:
- ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  ê¸€ìì™€ ìˆ«ìë¥¼ ì •í™•íˆ ì½ì–´ì£¼ì„¸ìš”
- ì¢Œí‘œë‚˜ ìœ„ì¹˜ ì •ë³´ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”
- ì½ëŠ” ìˆœì„œëŒ€ë¡œ í•œ ì¤„ì”© ì¶œë ¥í•´ì£¼ì„¸ìš”
- ì„¤ëª… ì—†ì´ í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”

Extract all visible text from this image. Return only the actual text content, not coordinates."""
    
    def _crop_image_intelligently(self, image_path, target_size=(1024, 1024)):
        """ì´ë¯¸ì§€ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ í¬ë¡­í•˜ì—¬ OCR ì„±ëŠ¥ í–¥ìƒ"""
        try:
            with Image.open(image_path) as img:
                width, height = img.size
                
                # ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í¬ë©´ ì¤‘ì•™ ë¶€ë¶„ì„ í¬ë¡­
                if width > target_size[0] * 2 or height > target_size[1] * 2:
                    # ì¤‘ì•™ì—ì„œ target_size í¬ê¸°ë¡œ í¬ë¡­
                    left = (width - target_size[0]) // 2
                    top = (height - target_size[1]) // 2
                    right = left + target_size[0]
                    bottom = top + target_size[1]
                    
                    # ê²½ê³„ ì²´í¬
                    left = max(0, left)
                    top = max(0, top)
                    right = min(width, right)
                    bottom = min(height, bottom)
                    
                    cropped = img.crop((left, top, right, bottom))
                    
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    temp_path = os.path.join(temp_dir, f"cropped_{os.path.basename(image_path)}")
                    cropped.save(temp_path)
                    
                    print(f"ğŸ” ì´ë¯¸ì§€ í¬ë¡­ë¨: {width}x{height} â†’ {cropped.size[0]}x{cropped.size[1]}")
                    return temp_path
                    
            return image_path  # í¬ë¡­ì´ í•„ìš”ì—†ìœ¼ë©´ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
            
        except Exception as e:
            print(f"âš ï¸  ì´ë¯¸ì§€ í¬ë¡­ ì‹¤íŒ¨: {e}")
            return image_path
    
    def process_image_hybrid(self, image_path):
        """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ê·¸ë¦¬ë“œ ê¸°ë°˜ ì˜ì—­ ë¶„í•  + AI ì²˜ë¦¬"""
        try:
            print(f"ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‹œì‘: {os.path.basename(image_path)}")
            print(f"ğŸ“ ë°©ì‹: ê·¸ë¦¬ë“œ ê¸°ë°˜ ì˜ì—­ ë¶„í•  (OpenCV ëŒ€ì²´)")
            
            # ê·¸ë¦¬ë“œ ê¸°ë°˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ import
            from PIL import Image
            import math
            
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ê¸°ë³¸ ì •ë³´
            img = Image.open(image_path)
            width, height = img.size
            
            # ì ì‘ì  ê·¸ë¦¬ë“œ í¬ê¸° ê²°ì • (12ê°œ ì˜ì—­ ëª©í‘œ)
            target_regions = 12
            aspect_ratio = width / height
            cols = math.ceil(math.sqrt(target_regions * aspect_ratio))
            rows = math.ceil(target_regions / cols)
            
            print(f"ğŸ” ì´ë¯¸ì§€ ë¶„í• : {cols}Ã—{rows} ê·¸ë¦¬ë“œ ({cols*rows}ê°œ ì˜ì—­)")
            
            # ê·¸ë¦¬ë“œ ì˜ì—­ ìƒì„±
            grid_width = width // cols
            grid_height = height // rows
            overlap_ratio = 0.1
            overlap_w = int(grid_width * overlap_ratio)
            overlap_h = int(grid_height * overlap_ratio)
            
            all_texts = []
            successful_regions = 0
            
            # ê° ì˜ì—­ ì²˜ë¦¬
            for row in range(rows):
                for col in range(cols):
                    try:
                        # ì˜ì—­ ì¢Œí‘œ ê³„ì‚°
                        start_x = max(0, col * grid_width - overlap_w)
                        start_y = max(0, row * grid_height - overlap_h)
                        end_x = min(width, (col + 1) * grid_width + overlap_w)
                        end_y = min(height, (row + 1) * grid_height + overlap_h)
                        
                        # ì˜ì—­ í¬ë¡­
                        region = img.crop((start_x, start_y, end_x, end_y))
                        
                        print(f"ğŸ¤– ì˜ì—­ ({row},{col}) ì²˜ë¦¬ ì¤‘...")
                        
                        # AIë¡œ ì˜ì—­ ì²˜ë¦¬
                        region_text = self._process_grid_region(region, row, col)
                        
                        if region_text and region_text.strip() and not region_text.startswith("ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜"):
                            # "ì—†ìŒ" ê°™ì€ ì‘ë‹µ í•„í„°ë§
                            if region_text.lower() not in ['ì—†ìŒ', 'none', 'no text', 'no circles', 'ì›í˜• ì—†ìŒ']:
                                all_texts.append(region_text.strip())
                                successful_regions += 1
                                print(f"âœ… ì˜ì—­ ({row},{col}): '{region_text.strip()[:30]}...'")
                            else:
                                print(f"âŒ ì˜ì—­ ({row},{col}): ì›í˜• í…ìŠ¤íŠ¸ ì—†ìŒ")
                        else:
                            print(f"âŒ ì˜ì—­ ({row},{col}): í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                            
                    except Exception as e:
                        print(f"âŒ ì˜ì—­ ({row},{col}) ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue
            
            # ê²°ê³¼ ì •ë¦¬
            if all_texts:
                final_result = "\n".join(all_texts)
                print(f"ğŸ‰ ê·¸ë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ: {successful_regions}/{cols*rows}ê°œ ì˜ì—­ ì„±ê³µ")
                print(f"ğŸ“ ì´ ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(final_result)}ì")
                return final_result
            else:
                print(f"âš ï¸  ëª¨ë“  ì˜ì—­ì—ì„œ ì›í˜• í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨. ì¼ë°˜ ëª¨ë“œë¡œ ëŒ€ì²´")
                return self._process_single_image_fallback(image_path, "general")
                
        except Exception as e:
            print(f"âŒ ê·¸ë¦¬ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return self._process_single_image_fallback(image_path, "general")
    
    def _process_grid_region(self, region_image, row, col):
        """ê·¸ë¦¬ë“œ ì˜ì—­ ê°œë³„ ì²˜ë¦¬"""
        try:
            import base64
            import io
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            img_buffer = io.BytesIO()
            region_image.save(img_buffer, format='PNG')
            img_buffer.seek(0)
            base64_image = base64.b64encode(img_buffer.read()).decode('utf-8')
            
            # ì›í˜•/íƒ€ì›í˜• ê°ì§€ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
            prompt_text = """ì´ ì´ë¯¸ì§€ ì˜ì—­ì—ì„œ ìˆ˜ê¸°ë¡œ ê·¸ì–´ì§„ ì›í˜•ì´ë‚˜ íƒ€ì›í˜• ë„í˜• ì•ˆì— ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì°¾ì•„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ğŸ¯ ì°¾ì•„ì•¼ í•  ê²ƒ:
- ì†ìœ¼ë¡œ ê·¸ë¦° ì›, íƒ€ì›, ë™ê·¸ë¼ë¯¸ ì•ˆì˜ í…ìŠ¤íŠ¸
- ì™„ì „í•˜ì§€ ì•Šì€ ì›í˜•ë„ í¬í•¨ (ì°Œê·¸ëŸ¬ì§„ ì›, íƒ€ì›)
- ì‘ì€ ì›ë¶€í„° í° ì›ê¹Œì§€ ëª¨ë“  í¬ê¸°
- ì†ê¸€ì”¨ë‚˜ ì¸ì‡„ í…ìŠ¤íŠ¸ ëª¨ë‘

âš ï¸ ë¬´ì‹œí•  ê²ƒ:
- ì‚¬ê°í˜•, ì‚¼ê°í˜•, ì§ì„ ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ í…ìŠ¤íŠ¸
- ì›í˜• ë°–ì— ìˆëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸
- í‘œë‚˜ ì„ ìœ¼ë¡œë§Œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸

ğŸ“ ì¶œë ¥ í˜•ì‹:
ì›í˜•/íƒ€ì›í˜• ì•ˆì— í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ í…ìŠ¤íŠ¸ë§Œ í•œ ì¤„ì”© ì¶œë ¥í•˜ê³ ,
ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³  ì¶œë ¥í•´ì£¼ì„¸ìš”.

Find text inside hand-drawn circles or ellipses only. Ignore rectangular boxes and plain text."""
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"image": f"data:image/png;base64,{base64_image}"},
                        {"text": prompt_text}
                    ]
                }
            ]
            
            response = dashscope.MultiModalConversation.call(
                model=self.model_name,
                messages=messages
            )
            
            # ìƒˆë¡œìš´ ì‘ë‹µ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
            from response_utils import extract_text_from_response
            result = extract_text_from_response(response)
            return result if result else "ì—†ìŒ"
            
        except Exception as e:
            return f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
    
    def _process_single_image_fallback(self, image_path, mode="general"):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (í•˜ì´ë¸Œë¦¬ë“œ ëŒ€ì²´ìš©)"""
        try:
            # ì´ë¯¸ì§€ ì¸ì½”ë”©
            base64_image = self._encode_image(image_path)
            if not base64_image:
                return "ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨"
            
            # ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
            prompt_text = self._get_prompt_for_mode(mode)
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"image": f"data:image/jpeg;base64,{base64_image}"},
                        {"text": prompt_text}
                    ]
                }
            ]
            
            response = dashscope.MultiModalConversation.call(
                model=self.model_name,
                messages=messages
            )
            
            # ìƒˆë¡œìš´ ì‘ë‹µ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
            result = extract_text_from_response(response)
            return result if result else "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
            
        except Exception as e:
            return f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
    
    @measure_time
    def process_image(self, image_path, mode=None):
        """ë‹¨ì¼ ì´ë¯¸ì§€ OCR ì²˜ë¦¬ - ëª¨ë“œë³„ ë° í¬ë¡­ ì§€ì›"""
        if mode is None:
            mode = self.ocr_mode
            
        max_retries = 3
        retry_delay = 2
        
        # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if mode == "hybrid":
            return self.process_image_hybrid(image_path)
        
        # ì›ë³¸ê³¼ í¬ë¡­ëœ ì´ë¯¸ì§€ ëª¨ë‘ ì‹œë„
        image_paths_to_try = [image_path]
        
        # ì´ë¯¸ì§€ê°€ í¬ë©´ í¬ë¡­ ë²„ì „ë„ ì‹œë„
        try:
            with Image.open(image_path) as img:
                if img.size[0] > 2048 or img.size[1] > 2048:
                    cropped_path = self._crop_image_intelligently(image_path)
                    if cropped_path != image_path:
                        image_paths_to_try.append(cropped_path)
        except:
            pass
        
        best_result = None
        best_length = 0
        
        for img_path in image_paths_to_try:
            for attempt in range(max_retries):
                try:
                    # ì´ë¯¸ì§€ ì¸ì½”ë”©
                    base64_image = self._encode_image(img_path)
                    if not base64_image:
                        continue
                    
                    # ëª¨ë“œë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
                    prompt_text = self._get_prompt_for_mode(mode)
                    
                    messages = [
                        {
                            "role": "user",
                            "content": [
                                {"image": f"data:image/jpeg;base64,{base64_image}"},
                                {"text": prompt_text}
                            ]
                        }
                    ]
                    
                    response = dashscope.MultiModalConversation.call(
                        model=self.model_name,
                        messages=messages
                    )
                    
                    # ìƒˆë¡œìš´ ì‘ë‹µ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
                    result = extract_text_from_response(response)
                    
                    # ê²°ê³¼ í’ˆì§ˆ í‰ê°€ (í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ê°„ë‹¨íˆ íŒë‹¨)
                    if result and len(result.strip()) > best_length:
                        best_result = result
                        best_length = len(result.strip())
                        
                        # ì¶©ë¶„íˆ ì¢‹ì€ ê²°ê³¼ë©´ ë°”ë¡œ ë°˜í™˜
                        if best_length > 10:  # 10ì ì´ìƒì´ë©´ ì¶©ë¶„
                            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                            if img_path != image_path and os.path.exists(img_path):
                                try:
                                    os.remove(img_path)
                                except:
                                    pass
                            
                            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œ ì‘ë‹µ êµ¬ì¡° ì¶œë ¥
                            if os.getenv('DEBUG_API_RESPONSE', '').lower() == 'true':
                                debug_response_structure(response)
                                
                            return result
                        
                except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸  ì—°ê²° ì˜¤ë¥˜ (ì¬ì‹œë„ {attempt + 1}/{max_retries}): {str(e)[:100]}...")
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                    else:
                        print(f"ì—°ê²° ì‹¤íŒ¨: {str(e)[:100]}...")
                        break
                        
                except Exception as e:
                    print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    break
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for img_path in image_paths_to_try:
            if img_path != image_path and os.path.exists(img_path):
                try:
                    os.remove(img_path)
                except:
                    pass
        
        # ìµœê³  ê²°ê³¼ ë°˜í™˜ ë˜ëŠ” ì‹¤íŒ¨
        if best_result:
            return best_result
        else:
            return "ì²˜ë¦¬ ì‹¤íŒ¨: í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    
    def process_images(self, image_files, output_base_dir):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = create_output_directory(output_base_dir, f"cloud_{self.model_name}")
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {output_dir}")
        print(f"ğŸ“Š ì²˜ë¦¬í•  ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")
        print(f"ğŸŒ ì‚¬ìš© ëª¨ë¸: {self.model_name}")
        
        total_time = 0
        successful_count = 0
        api_calls = 0
        results = []  # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        
        # ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=len(image_files), desc="ì´ë¯¸ì§€ ì²˜ë¦¬ì¤‘") as pbar:
            for image_path in image_files:
                filename = os.path.basename(image_path)
                pbar.set_postfix({"í˜„ì¬": filename})
                
                try:
                    # OCR ì²˜ë¦¬
                    result_text, process_time = self.process_image(image_path)
                    total_time += process_time
                    api_calls += 1
                    
                    # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
                    is_success = (
                        result_text and 
                        not result_text.startswith("API í˜¸ì¶œ ì‹¤íŒ¨") and 
                        not result_text.startswith("ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜") and
                        not result_text.startswith("ì—°ê²° ì‹¤íŒ¨") and
                        not result_text.startswith("Image encoding failed") and
                        result_text != "ì²˜ë¦¬ ì‹¤íŒ¨: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
                    )
                    
                    if is_success:
                        successful_count += 1
                        
                        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
                        text_file = save_text_result(result_text, output_dir, filename)
                        
                        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
                        base_name = os.path.splitext(filename)[0]
                        output_image_path = os.path.join(output_dir, f"{base_name}_result.png")
                        image_success = draw_text_on_image(image_path, result_text, output_image_path)
                        
                        # í…ìŠ¤íŠ¸ ì¢Œí‘œ ë§¤í•‘ ì´ë¯¸ì§€ ìƒì„±
                        try:
                            from text_coordinate_mapping import create_text_coordinate_mapping
                            coord_success = create_text_coordinate_mapping(
                                image_path, result_text, output_dir, method="auto"
                            )
                            if coord_success:
                                print(f"ğŸ¯ ì¢Œí‘œ ë§¤í•‘ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                        except Exception as coord_error:
                            print(f"âš ï¸  ì¢Œí‘œ ë§¤í•‘ ì˜¤ë¥˜: {coord_error}")
                        
                        results.append({
                            'file': filename,
                            'success': True,
                            'text_length': len(result_text),
                            'time': process_time
                        })
                        
                        print(f"âœ… {filename}: {len(result_text)}ì ì¶”ì¶œ")
                        
                    else:
                        print(f"âš ï¸  ì‹¤íŒ¨: {filename} - {result_text[:100]}...")
                        results.append({
                            'file': filename,
                            'success': False,
                            'error': result_text[:200] if result_text else "Unknown error",
                            'time': process_time
                        })
                
                except Exception as e:
                    print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {filename} - {str(e)}")
                    results.append({
                        'file': filename,
                        'success': False,
                        'error': str(e)[:200],
                        'time': 0
                    })
                
                # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ê³ ë ¤ (ì•½ê°„ì˜ ëŒ€ê¸°)
                time.sleep(0.5)  # SSL ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ì¡°ê¸ˆ ë” ê¸¸ê²Œ
                pbar.update(1)
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary_path = os.path.join(output_dir, "summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"=== í´ë¼ìš°ë“œ API ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\n")
            f.write(f"ëª¨ë¸: {self.model_name}\n")
            f.write(f"ì„±ê³µ: {successful_count}/{len(image_files)} ì´ë¯¸ì§€\n")
            f.write(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\n")
            f.write(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(image_files):.2f}ì´ˆ/ì´ë¯¸ì§€\n")
            f.write(f"API í˜¸ì¶œ ìˆ˜: {api_calls}\n\n")
            
            for result in results:
                f.write(f"íŒŒì¼: {result['file']}\n")
                f.write(f"ì„±ê³µ: {'ì˜ˆ' if result['success'] else 'ì•„ë‹ˆì˜¤'}\n")
                if result['success']:
                    f.write(f"ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {result['text_length']}ì\n")
                else:
                    f.write(f"ì˜¤ë¥˜: {result.get('error', 'Unknown')}\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {result['time']:.2f}ì´ˆ\n")
                f.write("-" * 30 + "\n")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
        print(f"ì„±ê³µ: {successful_count}/{len(image_files)} ì´ë¯¸ì§€")
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(image_files):.2f}ì´ˆ/ì´ë¯¸ì§€")
        print(f"API í˜¸ì¶œ ìˆ˜: {api_calls}")
        print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
        return True


def run_cloud_ocr(api_key, model_name, image_files, output_dir, ocr_mode="shape_detection"):
    """í´ë¼ìš°ë“œ OCR ì‹¤í–‰ í•¨ìˆ˜ - ëª¨ë“œ ì§€ì›"""
    if not api_key or api_key == "your_api_key_here":
        print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì—ì„œ QWEN_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return False
    
    processor = CloudOCRProcessor(api_key, model_name)
    
    # OCR ëª¨ë“œ ì„¤ì •
    processor.set_ocr_mode(ocr_mode)
    
    try:
        print(f"ğŸŒ Qwen Cloud API ì—°ê²° ì¤‘...")
        print(f"ğŸ“¡ ëª¨ë¸: {model_name}")
        print(f"ğŸ¯ OCR ëª¨ë“œ: {ocr_mode}")
        print("âš ï¸  ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œê°€ ìˆì„ ê²½ìš° ì¬ì‹œë„ë©ë‹ˆë‹¤...")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        success = processor.process_images(image_files, output_dir)
        return success
        
    except Exception as e:
        print(f"âŒ Cloud API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False
