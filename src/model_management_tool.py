#!/usr/bin/env python3
"""
ëª¨ë¸ ê´€ë¦¬ ë„êµ¬ - ë¡œë“œëœ ëª¨ë¸ í™•ì¸, ì •ë¦¬, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""

import os
import sys
import time
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(__file__))

def show_model_status():
    """í˜„ì¬ ëª¨ë¸ ìƒíƒœ í‘œì‹œ"""
    print("ğŸ“Š ëª¨ë¸ ë§¤ë‹ˆì € ìƒíƒœ")
    print("=" * 40)
    
    try:
        from model_manager import get_model_manager
        
        manager = get_model_manager()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_info = manager.get_memory_usage()
        print(f"ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"   ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {memory_info['loaded_models']}/{memory_info['max_models']}")
        print(f"   í˜„ì¬ í™œì„± ëª¨ë¸: {memory_info.get('current_model', 'None')}")
        
        if 'gpu_memory_allocated' in memory_info:
            print(f"   GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_info['gpu_memory_allocated']:.2f}GB")
            print(f"   GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_info['gpu_memory_reserved']:.2f}GB")
        
        # ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
        loaded_models = manager.list_loaded_models()
        if loaded_models:
            print(f"\nğŸ“‹ ë¡œë“œëœ ëª¨ë¸ ëª©ë¡:")
            for i, model_info in enumerate(loaded_models, 1):
                status = "ğŸŸ¢ í™œì„±" if model_info['is_current'] else "âšª ëŒ€ê¸°"
                print(f"   {i}. {status} {model_info['model_id']}")
                print(f"      ë””ë°”ì´ìŠ¤: {model_info['device']}")
                print(f"      ë§ˆì§€ë§‰ ì‚¬ìš©: {time.strftime('%H:%M:%S', time.localtime(model_info['last_used']))}")
                print()
        else:
            print("\nğŸ“ ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")

def clear_models():
    """ëª¨ë¸ ì •ë¦¬"""
    print("ğŸ§¹ ëª¨ë¸ ì •ë¦¬")
    print("=" * 20)
    
    try:
        from model_manager import get_model_manager
        
        manager = get_model_manager()
        loaded_models = manager.list_loaded_models()
        
        if not loaded_models:
            print("ğŸ“ ì •ë¦¬í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"í˜„ì¬ {len(loaded_models)}ê°œì˜ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        confirm = input("ëª¨ë“  ëª¨ë¸ì„ ì •ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        
        if confirm == 'y':
            manager.clear_all_models()
            print("âœ… ëª¨ë“  ëª¨ë¸ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("=" * 30)
    
    from models import list_local_models
    from model_manager import get_model_manager
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
    models = list_local_models()
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
    for i, (key, info) in enumerate(models.items(), 1):
        print(f"  {i}. {info['name']} ({info['params']})")
    
    # ëª¨ë¸ ì„ íƒ
    while True:
        try:
            choice = input(f"\ní…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (1-{len(models)}): ").strip()
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                model_keys = list(models.keys())
                selected_key = model_keys[idx]
                model_info = models[selected_key]
                break
            else:
                print(f"âŒ 1-{len(models)} ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except ValueError:
            print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    
    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”„ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸: {model_info['name']}")
    
    try:
        manager = get_model_manager()
        
        start_time = time.time()
        
        model, processor, device = manager.get_model(model_info['model_id'])
        
        load_time = time.time() - start_time
        
        print(f"âœ… ë¡œë”© ì„±ê³µ!")
        print(f"â±ï¸  ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
        print(f"ğŸ’¾ ë””ë°”ì´ìŠ¤: {device}")
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        memory_info = manager.get_memory_usage()
        if 'gpu_memory_allocated' in memory_info:
            print(f"ğŸ® GPU ë©”ëª¨ë¦¬: {memory_info['gpu_memory_allocated']:.2f}GB")
        
    except Exception as e:
        print(f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def change_settings():
    """ì„¤ì • ë³€ê²½"""
    print("âš™ï¸  ì„¤ì • ë³€ê²½")
    print("=" * 20)
    
    try:
        from model_manager import get_model_manager
        
        manager = get_model_manager()
        current_max = manager.max_models
        
        print(f"í˜„ì¬ ìµœëŒ€ ëª¨ë¸ ìˆ˜: {current_max}")
        print("ìµœëŒ€ ëª¨ë¸ ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ê¶Œì¥: 1-3)")
        
        while True:
            try:
                new_max = input(f"ìƒˆë¡œìš´ ìµœëŒ€ ëª¨ë¸ ìˆ˜ (1-5, í˜„ì¬: {current_max}): ").strip()
                new_max = int(new_max)
                
                if 1 <= new_max <= 5:
                    manager.change_max_models(new_max)
                    print(f"âœ… ìµœëŒ€ ëª¨ë¸ ìˆ˜ê°€ {new_max}ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                else:
                    print("âŒ 1-5 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
                
    except Exception as e:
        print(f"âŒ ì„¤ì • ë³€ê²½ ì‹¤íŒ¨: {e}")

def test_model_reuse():
    """ëª¨ë¸ ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸"""
    print("ğŸ”„ ëª¨ë¸ ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸")
    print("=" * 30)
    
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ë™ì¼í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ ë¡œë“œí•˜ì—¬")
    print("ë‘ ë²ˆì§¸ë¶€í„°ëŠ” ë¹ ë¥´ê²Œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.")
    
    from models import list_local_models
    from model_manager import get_model_manager
    
    # ê°€ì¥ ì‘ì€ ëª¨ë¸ ì„ íƒ
    models = list_local_models()
    smallest_model = None
    for key, info in models.items():
        if "2B" in info['params']:
            smallest_model = info
            break
    
    if not smallest_model:
        smallest_model = list(models.values())[0]  # ì²« ë²ˆì§¸ ëª¨ë¸
    
    print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {smallest_model['name']}")
    
    manager = get_model_manager()
    
    # ì²« ë²ˆì§¸ ë¡œë“œ
    print("\n1ï¸âƒ£  ì²« ë²ˆì§¸ ë¡œë“œ...")
    start_time = time.time()
    
    try:
        model, processor, device = manager.get_model(smallest_model['model_id'])
        first_load_time = time.time() - start_time
        print(f"âœ… ì²« ë²ˆì§¸ ë¡œë“œ ì™„ë£Œ: {first_load_time:.2f}ì´ˆ")
        
        # ë‘ ë²ˆì§¸ ë¡œë“œ (ì¬ì‚¬ìš©)
        print("\n2ï¸âƒ£  ë‘ ë²ˆì§¸ ë¡œë“œ (ì¬ì‚¬ìš©)...")
        start_time = time.time()
        
        model, processor, device = manager.get_model(smallest_model['model_id'])
        second_load_time = time.time() - start_time
        print(f"âœ… ë‘ ë²ˆì§¸ ë¡œë“œ ì™„ë£Œ: {second_load_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ë¹„êµ
        speedup = first_load_time / second_load_time if second_load_time > 0 else float('inf')
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"ì²« ë²ˆì§¸: {first_load_time:.2f}ì´ˆ")
        print(f"ë‘ ë²ˆì§¸: {second_load_time:.2f}ì´ˆ")
        print(f"ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° ë¹ ë¦„!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def performance_monitor():
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    print("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
    print("=" * 30)
    
    try:
        import torch
        import psutil
        from model_manager import get_model_manager
        
        manager = get_model_manager()
        
        print("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU ì‚¬ìš©ë¥ : {psutil.cpu_percent():.1f}%")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {psutil.virtual_memory().percent:.1f}%")
        
        if torch.cuda.is_available():
            print(f"\nğŸ® GPU ì •ë³´:")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
                
                print(f"   GPU {i}: {gpu_name}")
                print(f"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {memory_allocated:.2f}GB")
                print(f"   ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {memory_reserved:.2f}GB")
                print(f"   ì „ì²´ ë©”ëª¨ë¦¬: {memory_total:.2f}GB")
                print(f"   ì‚¬ìš©ë¥ : {(memory_reserved/memory_total)*100:.1f}%")
        else:
            print("\nâš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ë§¤ë‹ˆì € ì •ë³´
        memory_info = manager.get_memory_usage()
        print(f"\nğŸ§  ëª¨ë¸ ë§¤ë‹ˆì €:")
        print(f"   ë¡œë“œëœ ëª¨ë¸: {memory_info['loaded_models']}/{memory_info['max_models']}")
        
        loaded_models = manager.list_loaded_models()
        if loaded_models:
            for model_info in loaded_models:
                status = "ğŸ”´ í™œì„±" if model_info['is_current'] else "âšª ëŒ€ê¸°"
                print(f"   {status} {model_info['model_id'].split('/')[-1]} ({model_info['device']})")
        
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    while True:
        print("\nğŸ› ï¸  ëª¨ë¸ ê´€ë¦¬ ë„êµ¬")
        print("=" * 25)
        print("1. ëª¨ë¸ ìƒíƒœ í™•ì¸")
        print("2. ëª¨ë¸ ì •ë¦¬")
        print("3. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
        print("4. ëª¨ë¸ ì¬ì‚¬ìš© í…ŒìŠ¤íŠ¸")
        print("5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
        print("6. ì„¤ì • ë³€ê²½")
        print("7. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (1-7): ").strip()
        
        if choice == "1":
            show_model_status()
        elif choice == "2":
            clear_models()
        elif choice == "3":
            test_model_loading()
        elif choice == "4":
            test_model_reuse()
        elif choice == "5":
            performance_monitor()
        elif choice == "6":
            change_settings()
        elif choice == "7":
            print("ğŸ‘‹ ëª¨ë¸ ê´€ë¦¬ ë„êµ¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        else:
            print("âŒ 1-7 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì ì‹œ ëŒ€ê¸°
        input("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")

if __name__ == "__main__":
    main()
