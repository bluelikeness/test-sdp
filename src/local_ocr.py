"""
ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•œ OCR ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
"""

import torch
try:
    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
except ImportError:
    from transformers import AutoModelForCausalLM as Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

from utils import create_output_directory, draw_text_on_image, save_text_result, measure_time

class LocalOCRProcessor:
    def __init__(self, model_id, device="auto"):
        self.model_id = model_id
        self.device = self._get_device(device)
        self.model = None
        self.processor = None
        
    def _get_device(self, device):
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if device == "auto":
            if torch.cuda.is_available():
                try:
                    # GPU ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸
                    torch.cuda.empty_cache()
                    test_tensor = torch.tensor([1.0]).cuda()
                    del test_tensor
                    torch.cuda.empty_cache()
                    print("âœ… GPU ì‚¬ìš© ê°€ëŠ¥")
                    return "cuda"
                except Exception as e:
                    print(f"âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ ({e}), CPUë¡œ ëŒ€ì²´")
                    return "cpu"
            else:
                print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ, CPUë¡œ ì‹¤í–‰")
                return "cpu"
        return device
    
    @measure_time
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_id}")
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
        
        if self.device == "cpu":
            print("âš ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("â³ ëª¨ë¸ ë¡œë”©ì— ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        
        try:
            # í”„ë¡œì„¸ì„œ ë¨¼ì € ë¡œë“œ
            print("ğŸ“¦ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_id,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ§  ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # ë””ë°”ì´ìŠ¤ë³„ ì„¤ì •
            if self.device == "cpu":
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.model_id,
                    torch_dtype=torch.float32,
                    device_map=None,
                    trust_remote_code=True
                )
                self.model = self.model.to("cpu")
            else:
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.model_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            # CPU ëª¨ë“œë¡œ ì¬ì‹œë„
            if self.device == "cuda":
                print("â™¾ï¸ CPU ëª¨ë“œë¡œ ì¬ì‹œë„...")
                self.device = "cpu"
                return self.load_model()
            
            return False
    
    @measure_time 
    def process_image(self, image_path):
        """ë‹¨ì¼ ì´ë¯¸ì§€ OCR ì²˜ë¦¬"""
        try:
            if self.device == "cpu":
                print(f"â³ CPU ëª¨ë“œë¡œ ì²˜ë¦¬ ì¤‘: {os.path.basename(image_path)} (ëŠë¦´ ìˆ˜ ìˆìŒ)")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')
            
            # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            prompt = "Find all text that has been manually circled with oval/elliptical pen marks and extract only those text items. Output only the results without any explanation or additional text."
            
            # Qwen2-VL ì „ìš© ì…ë ¥ í˜•ì‹
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì²˜ë¦¬
            text = self.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                text=[text],
                images=[image], 
                padding=True,
                return_tensors="pt"
            )
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if self.device == "cuda":
                inputs = inputs.to("cuda")
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=False,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            result = output_text.strip()
            
            if self.device == "cpu":
                print(f"âœ… CPU ì²˜ë¦¬ ì™„ë£Œ: {len(result)}ì ì¶”ì¶œ")
            
            return result
            
        except Exception as e:
            error_msg = f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            return error_msg
    
    def process_images(self, image_files, output_base_dir):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
        if not self.model:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        model_name = self.model_id.split('/')[-1]
        output_dir = create_output_directory(output_base_dir, f"local_{model_name}")
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {output_dir}")
        print(f"ğŸ“Š ì²˜ë¦¬í•  ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")
        
        total_time = 0
        successful_count = 0
        results = []
        
        # ì§„í–‰ë¥  í‘œì‹œ
        with tqdm(total=len(image_files), desc="ì´ë¯¸ì§€ ì²˜ë¦¬ì¤‘") as pbar:
            for i, image_path in enumerate(image_files):
                filename = os.path.basename(image_path)
                pbar.set_postfix({"í˜„ì¬": filename})
                
                try:
                    # OCR ì²˜ë¦¬
                    result_text, process_time = self.process_image(image_path)
                    total_time += process_time
                    
                    # ê²°ê³¼ ì €ì¥
                    if result_text and not result_text.startswith("ì²˜ë¦¬ ì‹¤íŒ¨"):
                        successful_count += 1
                        
                        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
                        text_file = save_text_result(result_text, output_dir, filename)
                        
                        # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
                        base_name = os.path.splitext(filename)[0]
                        output_image_path = os.path.join(output_dir, f"{base_name}_result.png")
                        success = draw_text_on_image(image_path, result_text, output_image_path)
                        
                        # í…ìŠ¤íŠ¸ ì¢Œí‘œ ë§¤í•‘ ì´ë¯¸ì§€ ìƒì„±
                        try:
                            from text_coordinate_mapping import create_text_coordinate_mapping
                            coord_success = create_text_coordinate_mapping(
                                image_path, result_text, output_dir, method="auto"
                            )
                            if coord_success:
                                print(f"ğŸ¯ ì¢Œí‘œ ë§¤í•‘ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                        except Exception as coord_error:
                            print(f"âš ï¸  ì¢Œí‘œ ë§¤í•‘ ì˜¤ë¥˜: {coord_error}")
                        
                        results.append({
                            'file': filename,
                            'success': True,
                            'text_length': len(result_text),
                            'time': process_time
                        })
                        
                        print(f"âœ… {filename}: {len(result_text)}ì ì¶”ì¶œ")
                        
                    else:
                        print(f"âš ï¸  ì‹¤íŒ¨: {filename} - {result_text}")
                        results.append({
                            'file': filename,
                            'success': False,
                            'error': result_text,
                            'time': process_time
                        })
                
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜: {filename} - {str(e)}")
                    results.append({
                        'file': filename,
                        'success': False,
                        'error': str(e),
                        'time': 0
                    })
                
                pbar.update(1)
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary_path = os.path.join(output_dir, "summary.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"=== ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ===\n")
            f.write(f"ì„±ê³µ: {successful_count}/{len(image_files)} ì´ë¯¸ì§€\n")
            f.write(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ\n")
            f.write(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(image_files):.2f}ì´ˆ/ì´ë¯¸ì§€\n\n")
            
            for result in results:
                f.write(f"íŒŒì¼: {result['file']}\n")
                f.write(f"ì„±ê³µ: {'ì˜ˆ' if result['success'] else 'ì•„ë‹ˆì˜¤'}\n")
                if result['success']:
                    f.write(f"ì¶”ì¶œ í…ìŠ¤íŠ¸ ê¸¸ì´: {result['text_length']}ì\n")
                else:
                    f.write(f"ì˜¤ë¥˜: {result.get('error', 'Unknown')}\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {result['time']:.2f}ì´ˆ\n")
                f.write("-" * 30 + "\n")
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n=== ì²˜ë¦¬ ì™„ë£Œ ===")
        print(f"ì„±ê³µ: {successful_count}/{len(image_files)} ì´ë¯¸ì§€")
        print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {total_time/len(image_files):.2f}ì´ˆ/ì´ë¯¸ì§€")
        print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
        return True
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.model:
            del self.model
            self.model = None
        if self.processor:
            del self.processor 
            self.processor = None
            
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def run_local_ocr(model_info, image_files, output_dir):
    """ë¡œì»¬ OCR ì‹¤í–‰ í•¨ìˆ˜"""
    processor = LocalOCRProcessor(model_info["model_id"])
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        success, load_time = processor.load_model()
        if not success:
            return False
        
        print(f"â±ï¸  ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.2f}ì´ˆ")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬
        success = processor.process_images(image_files, output_dir)
        return success
        
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        processor.cleanup()
